E12 - Gradient Boosting Review
XGBoosting vs GBoosting. A pesar de que el algoritmo base para ambos casos es el mismo implementando arboles de decisión con aumento de gradiente, donde se busca corregir los errores de modelos anteriores minimizando la perdida al incorporar nuevos modelos. Una de las principales diferencias es que XGBoost encuentra la mejor división para hacer en cada árbol, mediante la implementación de una expansión de Taylor simplificando la función de costo y haciendo más eficiente el procesamiento.
Otra de las principales diferencias es que XGBoost está enfocado a la velocidad computacional y rendimiento del modelo, permitiendo además, la Paralelización buscando utilizar todos los núcleos de la CPU durante el entrenamiento, computación distribuida para utilizar un grupo de máquinas cuando se tengan modelos muy grandes y la computación fuera del núcleo para cuando el conjunto de muestras es muy grande y no cabe en la memoria.
Por último, otra diferencia es que XGBoost ejecuta la regularización del árbol para evitar el Overfitting y adicionalmente trata a los missing values de manera más eficiente.
También en diferentes artículos mencionan que XGBoost tiene características adicionales como la penalización inteligente de árboles, reducción proporcional de ramas o nudos foliares y una aleatorización de parámetros adicional , logrando generar una reducción significativa de correlación entre los arboles .
En resumen, XGBoost es muy popular en las competencias de Kaggle debido a sus dos principales características, eficiencia computacional y rendimiento del modelo. Debido a su fácil parametrización y ventajas con el tratamiento de la información . 
